---
title: "Breast Cancer Type Prediction using Proteomic Data"
author: "Ngoc Duong - nqd2000"
date: "05/15/2020"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(tidyverse)
library(data.table)
library(viridis)
library(mgcv)
library(ggplot2)
library(pdp)
library(patchwork)
library(janitor)
library(ModelMetrics)
library(caret)
library(lime)
library(gplots)
library(microbenchmark)
library(RColorBrewer)
library(factoextra)
library(broom)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Introduction 
Lymph nodes are small, bean-shaped organs that act as filters along the lymph fluid channels. As lymph fluid leaves the breast and eventually goes back into the bloodstream, the lymph nodes try to catch and trap cancer cells before they reach other parts of the body. If breast cancer spreads, it typically goes first to nearby lymph nodes under the arm. It can also sometimes spread to lymph nodes near the collarbone or near the breast bone. Knowing if the cancer has spread to your lymph nodes helps doctors find the best way to treat your cancer. In other words, having cancer cells in the lymph nodes under your arm suggests an increased risk of the cancer spreading.When lymph nodes are free, or clear, of cancer, the test results are negative. If lymph nodes have some cancer cells in them, they are called positive (American Cancer Society)

In this project, we aim to use proteomic data and classification tools in machine learning to identify proteins that play a key role in the involvement of cancer cells in lymph nodes and prognosis of breast cancer in general. The result might have implications in precision medicine, where medications can be made or modified on an individual/sub-group level to regulate expression levels of proteins preventive or contributive to the spread of cancer cells to lymph nodes.

# Data cleaning 

The dataset we used contains iTRAQ proteome profiling of 80 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). Predictor variables are expression levels of about 12,000 proteins. Our main clinical outcome is the status of lymph-node involvement -- "positive" for cancer having spread to lymph nodes, and "negative" for cancer-free lymph nodes. 

**Import data and get rid of NA values**

The data obtained was quite raw, with missing values present when a given protein could not be quantified for a certain subject. For the simplicity of this project, we decided to drop all proteins that have missing values in them, even though these missing values were likely not random. This leaves us with about 8,000 proteins, The next step in the variable screening process involves selecting 5000 proteins with highest variances. Finally, we picked 500 proteins that were most associated with the outcome. Here, we used two-sample independent t-test to obtain a p-value for each protein, and then extracted 500 proteins with the lowest p-values as our final set of predictors. Before doing EDA and model building, we also scaled the data. 

```{r}
#import data
outcome = read.csv("./breastcancerproteomes/clinical_data_breast_cancer.csv") %>% janitor::clean_names()
proteome = read.csv("./breastcancerproteomes/77_cancer_proteomes_CPTAC_itraq.csv") 

#clean proteome data
#transpose dataset
proteome_tp <- transpose(proteome)

#get row and colnames in order
colnames(proteome_tp) <- proteome$RefSeq_accession_number
proteome_tp$par_id <- colnames(proteome) 

#rearrange data
proteome_with_id = as_tibble(proteome_tp) %>% select(par_id, everything()) %>% .[-c(1:3),] %>% separate(par_id, c("id2","id4","tcga")) %>% select(-tcga)

proteome_wo_id = as_tibble(proteome_tp) %>% select(-par_id, everything()) %>% .[-c(1:3),-ncol(proteome_tp)]
  
#clean outcome data 
outcome_clean = outcome %>% 
  separate(complete_tcga_id, c("tcga","id2","id4"), "-") %>% #sep id based on 2-digit id and 4-digit id
  select(-tcga) %>% 
  select(id2, id4, node_coded)

bcp_merge = left_join(proteome_with_id, outcome_clean, by = c("id2","id4")) %>% select(-id2, -id4) %>% drop_na(node_coded)
```
 
**Leave out proteins that have missing quantification values**

```{r}
missing.counts <- NULL
for(i in 1:ncol(proteome_wo_id)) {
missing.counts[i] <- sum(is.na(proteome_wo_id[,i]))}

miss <- names(proteome_wo_id)[which(missing.counts >= 1)] 

proteome_woid_nona = proteome_wo_id[-c(81:83),!(colnames(proteome_wo_id) %in% c(miss))]
```

**Screening variables**

```{r}
#find variance for all proteins
var <- NULL
for(i in 1:ncol(proteome_woid_nona)){
var[i] <- var(proteome_woid_nona[,i])}

var_id = tibble(row = c(1:7994), var) %>% arrange(desc(var)) %>% slice(1:5000)

#find names of proteins with high variances and put them in a vector
sup.var = names(proteome_woid_nona)[var_id$row]

#subset original data with the created vector
proteome_final = proteome_woid_nona[,c(sup.var)] %>% mutate_all(as.numeric) %>% as.matrix()

#use t-test to find proteins that are most associatd with the outcome
t_test = NULL
for(i in 1:ncol(proteome_final)){
  t_test[i] = t.test(proteome_final[,i]~bcp_merge$node_coded)$p.value
}

t_test_id = tibble(row = c(1:5000), t_test) %>% arrange(t_test) %>% slice(1:500)

low.pval = names(as_tibble(proteome_final))[t_test_id$row]

proteome_final2 = as_tibble(proteome_final) 
proteome_final3 = proteome_final2[,c(low.pval)] %>% mutate_all(as.numeric) 
```


**Obtain final data**

```{r}
#merge proteome and clinical data 
bcp_data = cbind(proteome_final3, node = bcp_merge$node_coded) %>% as_tibble() %>% drop_na(node) %>% mutate_at(vars(-node), as.numeric) 
```

# Exploratory data analysis

## Some unsupervised learning

```{r}
#get design matrix and scale for hclust
bcp_eda = bcp_data %>% select(-node)
bcp_eda1 = scale(bcp_eda)
```

**Heatmap based on hierarchical clustering**

```{r}
col1 = colorRampPalette(brewer.pal(9, "GnBu"))(80)
col2 = colorRampPalette(brewer.pal(3, "Spectral"))(2)

heatmap.2(t(bcp_eda1), col = col1, keysize = 0.7, key.par = list(cex = 0.5),
          trace = "none", key = TRUE, cexCol = 0.75,
          labcol = as.character(c(1:80)),
          ColSideColors = col2[as.numeric(bcp_data$node)],
          margins = c(10,10))
```

The heatmap (**Figure...**) shows some preliminary findings. As green represents "positive" and orange represents "negative" outcome, we can see that upregulation of some proteins are observed in people with positive cancer spread to lymphnode. These proteins include NP_004439, NP_653190

**k-means**
Use function fviz_nbclust to determine the optimal number of clusters using average sillhouette.

```{r}
fviz_nbclust(bcp_eda1, 
             FUNcluster = kmeans,
             method = "silhouette")
```


```{r}
#make clusters
set.seed(13)
km = kmeans(bcp_eda1, centers = 7, nstart = 30)

fviz_cluster(list(data = bcp_eda1, cluster = km$cluster),
                      ellipse.type = "convex",
                      geom = c("point","text"),
                      labelsize = 10,
                      palette  = "Dark2") + 
  labs(title = "K-means clustering for chosen k = 7") + theme_bw()
```

**Grouped boxplots**

```{r}
#divide obs into clusters based on k-means results
bcp_kmeans = bcp_data %>% mutate(id = c(1:80),
                                 cluster = ifelse(id %in% c(1,66,10,6,56,30,62,2,68,45,33), "cluster 4", ifelse(id %in% c(71, 49, 25, 46, 13, 47, 3, 74), "cluster 7", ifelse(id %in% c(41, 26, 19, 78, 42, 12, 37, 51, 59, 2, 20, 28, 27, 21, 20), "cluster 6", ifelse(id %in% c(52, 53,54, 55, 31, 27, 40, 14, 34, 17, 35, 42), "cluster 3", "other clusters"))))) %>% dplyr::select(c(1:25, 501), cluster)

#scale 
bcp_kmeans[, c(1:25)] <- scale(bcp_kmeans[, c(1:25)])

#grouped boxplot for top 25 proteins with highest variance
bcp_kmeans %>% 
  filter(cluster %in% c("cluster 3", "cluster 4", "cluster 6", "cluster 7")) %>% 
  pivot_longer(1:25,
               names_to = "protein",
               values_to = "value") %>% 
  group_by(node, cluster) %>% 
  ggplot(aes(x = protein, y = value, color = node)) +
  geom_boxplot() + theme_bw() + 
  labs(y = "Expression levels", x = "Protein") + 
  theme(legend.position = "right",
        axis.text.x = element_text(angle = 30, hjust = 1)) + 
  facet_grid(cluster~.)
```


# Prediction

```{r}
#create training set with a random sample of 800 observations
set.seed(13)
rowTrain <-createDataPartition(y = bcp_data$node,
                               p = 0.80,
                               list = FALSE)
bcp_train = bcp_data[rowTrain,]
bcp_test = bcp_data[-rowTrain,]
```


### Random Forest

```{r}
ctrl_cl = trainControl(method = "repeatedcv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

rf.grid = expand.grid(mtry = 2:30, 
                      splitrule = "gini",
                      min.node.size = 1:6)

set.seed(13)
rf.fit <- train(node~., bcp_train,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                importance = "permutation", 
                trControl = ctrl_cl)

ggplot(rf.fit, highlight = TRUE)
```

Variable importance

```{r}
varimp = varImp(rf.fit)$importance
varimp$protein = rownames(varimp)
varimp = as_tibble(varimp) %>% arrange(desc(Overall)) %>% slice(1:75)
```

**Hierarchical clustering**

We used function **varImp** to identify 75 most impactful proteins (using the selected Random Forest model) and used hierarchical clustering to get some information about the physiological functions and group memberships of these proteins. In the dendorgram (**Figure ...**), we decided to go with 9 clusters in order to get more similarities between proteins within the clusters. Ideally, in the absense of time and effort constraint, we would have tried increasing the number of clusters, thereby reducing height, to obtain better-defined protein groups.


```{r}
#Prepare data for hierarchical clustering 
bcp_hclust = bcp_eda1[,c(varimp$protein)]

#specify clustering types
hc.complete <-hclust(dist(t(bcp_hclust)), method = "complete")
hc.average <-hclust(dist(t(bcp_hclust)), method = "average")
hc.single <-hclust(dist(t(bcp_hclust)), method = "single")
```


```{r}
#give names to proteins 
hc.complete$labels = as.vector(varimp$protein)

fviz_dend(hc.complete, 
          k = 9, cex = 0.5, palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, 
          rect_border = "jco",
          labels_track_height = 2.5, horiz = TRUE)
```


Random Forest prediction results 

**Train error rate**

```{r}
rf.pred.train = predict(rf.fit, bcp_train, type = "prob")
rf.pred.res.train = ifelse(rf.pred.train$Negative > 0.5,'Negative','Positive')
table(rf.pred.res.train, bcp_train$node)
```

Error rate = (`r table(rf.pred.res.train, bcp_train$node)[2]` + `r table(rf.pred.res.train, bcp_train$node)[3]`)/68 = `r (table(rf.pred.res.train, bcp_train$node)[2] + table(rf.pred.res.train, bcp_train$node)[3])/68`

**Test error rate**

```{r}
rf.pred.test = predict(rf.fit, bcp_test, type = "prob")
rf.pred.res.test = ifelse(rf.pred.test$Negative > 0.5,'Negative','Positive')
table(rf.pred.res.test, bcp_test$node)
```

Error rate = (`r table(rf.pred.res.test, bcp_test$node)[2]` + `r table(rf.pred.res.test, bcp_test$node)[3]`)/12 = `r (table(rf.pred.res.test, bcp_test$node)[2] + table(rf.pred.res.test, bcp_test$node)[3])/12`

## Support vector classifier/machine

### Fit a support vector classifier (linear kernel) to the training data with Tumor Type as the response

```{r}
ctrl <-trainControl(method = "repeatedcv", 
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

set.seed(13)
svml.fit <-train(node~.,data = bcp_train,
                 method = "svmLinear2",
                 allowParallel = TRUE,
                 metric = "ROC",
                 preProc = c("scale", "center"),
                 tuneGrid =data.frame(cost =exp(seq(-8,-3,len=10))),
                 trControl = ctrl)

ggplot(svml.fit, highlight = TRUE)
```

**Find the training and test error rate**

Training error rate

```{r}
pred.svml.train <-predict(svml.fit, newdata = bcp_train)
caret::confusionMatrix(data = pred.svml.train, reference = bcp_train$node)
```

Test error rate 

```{r}
pred.svml.test <-predict(svml.fit, newdata = bcp_test)
caret::confusionMatrix(data = pred.svml.test, reference = bcp_test$node)
```

### Fit a support vector machine (radial kernel) to the training data with Tumor Type as the response

```{r}
svmr.grid <-expand.grid(C =exp(seq(-10,0,len=10)),
                        sigma =exp(seq(-8,1,len=10)))

set.seed(13)
svmr.fit <-train(node~.,data = bcp_train,
                 method = "svmRadial",
                 allowParallel=TRUE,
                 preProc = c("scale", "center"),
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

ggplot(svmr.fit, highlight = TRUE)
```

**Find the training and test error rate**

Training error rate

```{r}
pred.svmr.train <-predict(svmr.fit, newdata = bcp_train)
caret::confusionMatrix(data = pred.svmr.train, reference = bcp_train$node)
```

Test error rate

```{r}
pred.svmr.test <-predict(svmr.fit, newdata = bcp_test)
caret::confusionMatrix(data = pred.svmr.test, reference = bcp_test$node)
```

Given very close cross-validated accuracy and AUC, we decided to pick the support vector classifer with linear kernel even though the radial kernel model performs better on the test set. We suspect this surprisingly good test performance (93.75% accuracy) by SVM could be due to luck due to lower training accuracy (92%). In addition, since p > n, linear kernel should be sufficient to find hyperplanes that separate the classes well, while also robust to the caveat of high dimensionality. On the other hand, solving the radial kernel of SVM took more time and could be prone to overfitting. 

Although SVC and RF have very similar performance, we decided to go with the RF model for interpretation purpose (we have not had time to explore how to get varImp/most important support vectors on SVM).

## Visualizations of black-box models

After selecting the final model, we are interested in the global interpretation of some influential proteins in this model. 

First, we want to explore the marginal impact of some particular proteins on the predicted probability of lymph-node involvement across all observations. We extracted the 4 most important proteins (NP_003106, NP_808818, NP_653190, NP_061119), and created Partial Dependence Plots (PDPs)

### PDP curves

```{r warning = FALSE, message = FALSE}
pdp.p1 = rf.fit %>% 
  partial(pred.var = "NP_003106",
          grid.resolution = 100,
          prob = TRUE) %>% 
  autoplot(rug = TRUE, train = bcp_train, ylab = "Predicted probability") + 
  ggtitle("Random Forest")

pdp.p2 = rf.fit %>% 
  partial(pred.var = "NP_808818",
          grid.resolution = 100,
          prob = TRUE) %>% 
  autoplot(rug = TRUE, train = bcp_train, ylab = "")

pdp.p3 = rf.fit %>% 
  partial(pred.var = "NP_653190",
          grid.resolution = 100,
          prob = TRUE) %>% 
  autoplot(rug = TRUE, train = bcp_train, ylab = "Predicted probability")

pdp.p4 = rf.fit %>% 
  partial(pred.var = "NP_061119",
          grid.resolution = 100,
          prob = TRUE) %>% 
  autoplot(rug = TRUE, train = bcp_train, ylab = "")

grid.arrange(pdp.p1, pdp.p2, pdp.p3, pdp.p4, ncol = 2, nrow = 2)
```

From **Figure...**, we can see that higher expressions of proteins NP_003106, NP_653190, and NP_061119 are associated with lowered predicted probability of lymph-node involvement, on average. Meanwhile, upregulation of protein NP_808818 is related to higher expected risk of
lymph-node involvement, on average. 

to overexpressed in prostate cancer protects cancer cells from endoplasmic reticulum stress conferring a growth advantage.


### ICE curves

```{r wanring = FALSE, message = FALSE}
ice.rf.p1 = rf.fit %>% 
  partial(pred.var = "NP_003106",
          grid.resolution = 20,
          ice = TRUE) %>% 
  autoplot(alpha = 0.5, train = bcp_train, center = TRUE, ylab = "Predicted probability") + 
  ggtitle("Random Forest, centered") 

ice.rf.p2 = rf.fit %>% 
  partial(pred.var = "NP_808818",
          grid.resolution = 100,
          ice = TRUE) %>% 
  autoplot(alpha = 0.5, train = bcp_train, center = TRUE, ylab = "") 

ice.rf.p3 = rf.fit %>% 
  partial(pred.var = "NP_653190",
          grid.resolution = 100,
          ice = TRUE) %>% 
  autoplot(alpha = 0.5, train = bcp_train, center = TRUE, ylab = "Predicted probability")

ice.rf.p4 = rf.fit %>% 
  partial(pred.var = "NP_061119",
          grid.resolution = 100,
          ice = TRUE) %>% 
  autoplot(alpha = 0.5, train = bcp_train, center = TRUE, ylab = "")

grid.arrange(ice.rf.p1, ice.rf.p2, ice.rf.p3, ice.rf.p4, ncol = 2, nrow = 2)
```

While PDP shows the average effect of a feature on the response at a global scale, we could also focus on specific individual instances using the (centered) individual conditional expectation (ICE) plot. Each ICE line demonstrates one subject, which displays how the predicted probability of lymph node cancer spread changes as a protein expression level changes for that one subject. The centered ICE plots demonstrate quite similar interpretations to those of the PDPs. For instance, we identify an sharp increase in predicted risk as expression value of NP_808818 approaches -1, followed by a flatter increase in predicted risk once the protein is up-regulated from around 0 to 4. Futhermore, we see a couple turbulent regions between -2 and -1 and 0 and 2, but the overall trend is not affected much.

Although PDP and ICE curves were helpful in showing the patterns of influence of certain proteins on the response, they do not give insights into the impact of these variables when we make predictions on new observations. Therefore, we also used local Interpretable Model-agnostic Explanations (LIME) to see which proteins are important in explaining individual predictions and what their impacts are in these predictions.

**Visualization of explanations for each case and label combination in an explanation**

```{r}
explainer.rf = lime(bcp_train, rf.fit)
explanation.rf = explain(bcp_test[c(1,3,4,7),], explainer.rf, 
                         n_features = 7, labels = "Positive")
plot_features(explanation.rf)
```

LIME assumes linearity on a local scale (around any particular observation) for every complicated model. The visualization works by fitting a simple model (e.g., regression) on the permuted data (values change slightly) in the locality to mimic how the global model behaves at that locality. Specifically, looking at **Figure ...**, for case 1, we noticed that upregulation of NP_037481, NP_783859, and NP_068766 has negative effect on the expected risk of lymph-node involvement. These proteins carried the most weights in deciding outcome for the first case, but were not among the top 4 proteins explored above. Globally influential proteins NP_808818, NP_653190, NP_061119 were still among the most important in making prediction for cases 1-3. Their impacts on the predicted risk are in accordance with the global interpretations above.

### Limitations

The very sample size (n = 80) given high number of predictors (p = 500) produced some challenges for some models (such as regularized logistic regression or kNN). PCA could have been used for dimension-reduction pre-model training; however, the goal of interpreting our results on a protein-specific level diverted us from doing so. 

GBM and ADAboost likely would have performed better provided more computional power and time put in tuning the hyperparameters.

Leaving out proteins that had missing values instead of imputing might have caused more bias in the prediction results. However, for this particular problem, choosing among different imputation methods is another question to investigate. This is a caveat in this project that leaves much room to improve.

### APPENDIX

**Protein clusters**


**CV performance plots**


